{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance\n",
    "\n",
    "Creating a baseline to compare against current model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mitchell/projects/personal/f1-rating-system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitchell/Library/Caches/pypoetry/virtualenvs/f1-rating-system-bj5GO79q-py3.11/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Move project root folder\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"params.yaml\") as conf_file:\n",
    "    CONFIG = yaml.safe_load(conf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model: Elo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EloRatingSystem():\n",
    "    '''Baseline rating system for F1 drivers'''\n",
    "\n",
    "    def __init__(self, k: float, c: float):\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "\n",
    "    def get_win_prob(self, rating_a: float, rating_b: float) -> float:\n",
    "        '''Returns the win probability of driver-constructor A over driver-constructor B'''\n",
    "        return 1 / (1 + np.exp(-(rating_a - rating_b) / self.c))\n",
    "    \n",
    "    def get_rating_change(self, rating_change: float) -> float:\n",
    "        '''Returns updated driver rating'''\n",
    "        return self.k* rating_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            2     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.64508D-01    |proj g|=  3.12200D-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  6.49685D-01    |proj g|=  2.78665D-03\n",
      "\n",
      "At iterate    2    f=  6.36940D-01    |proj g|=  2.42540D-03\n",
      "\n",
      "At iterate    3    f=  6.17563D-01    |proj g|=  1.26630D-03\n",
      "\n",
      "At iterate    4    f=  6.14379D-01    |proj g|=  1.78302D-04\n",
      "\n",
      "At iterate    5    f=  6.14354D-01    |proj g|=  1.10378D-04\n",
      "\n",
      "At iterate    6    f=  6.14341D-01    |proj g|=  8.17124D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    2      6     14      1     0     0   8.171D-06   6.143D-01\n",
      "  F =  0.61434076350413436     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "# column indexes\n",
    "DRI_IX = 5 # driver id\n",
    "POS_IX = 6 # driver position\n",
    "\n",
    "MOD_DF = pd.read_csv(CONFIG[\"data\"][\"features_path\"])\n",
    "MOD_DF[[\"constructorScore\", \"driverScore\", \"expected\", \"actual\"]] = None\n",
    "IX_CHUNKS = MOD_DF.reset_index().groupby([\"year\", \"round\"])[\"index\"].agg([\"min\", \"max\"]).values\n",
    "MOD_MAT = MOD_DF.values\n",
    "\n",
    "DRI_RTG = {dri: CONFIG[\"model\"][\"start_score\"] for dri in set(MOD_DF[\"driverId\"])}\n",
    "\n",
    "def model_data(params: dict) -> float:\n",
    "    '''Returns mean negative log likelihood of the rating system. If\n",
    "    export = True, also exports results for data reporting.'''\n",
    "\n",
    "    dri_scores = DRI_RTG.copy()\n",
    "    log_likelihood = 0\n",
    "    n_pred = 0\n",
    "    model = EloRatingSystem(params[0], params[1])\n",
    "\n",
    "    for start_ix, end_ix in IX_CHUNKS:\n",
    "        yr_mat = MOD_MAT[start_ix:end_ix+1]\n",
    "\n",
    "        rnd_dri_scores = {dri: 0 for dri in yr_mat[:, DRI_IX]}\n",
    "\n",
    "        for ix_1, ix_2 in itertools.combinations(range(yr_mat.shape[0]), 2):\n",
    "            dri_a, pos_a = yr_mat[ix_1, [DRI_IX, POS_IX]]\n",
    "            dri_b, pos_b = yr_mat[ix_2, [DRI_IX, POS_IX]]\n",
    "    \n",
    "            # continue if drivers in same car or a driver does not finish for misc reason\n",
    "            if pos_a == pos_b:\n",
    "                continue\n",
    "\n",
    "            # get current rating\n",
    "            elo_a = dri_scores[dri_a]\n",
    "            elo_b = dri_scores[dri_b]\n",
    "            \n",
    "            # create expected scores\n",
    "            e_a = model.get_win_prob(elo_a, elo_b)\n",
    "            e_b = 1 - e_a\n",
    "\n",
    "            # create true scores and track log likelihood \n",
    "            if pos_a < pos_b:\n",
    "                o_a = 1\n",
    "                log_likelihood += np.log(max(e_a, 1E-10))\n",
    "\n",
    "            else:\n",
    "                o_a = 0\n",
    "                log_likelihood += np.log(max(e_b, 1E-10))\n",
    "\n",
    "            n_pred += 1\n",
    "                \n",
    "            # calculate score change and update round scores\n",
    "            diff_a = o_a - e_a\n",
    "            diff_b = -diff_a\n",
    "\n",
    "            # log driver score changes per round\n",
    "            rnd_dri_scores[dri_a] += diff_a\n",
    "            rnd_dri_scores[dri_b] += diff_b\n",
    "        \n",
    "        # update driver values for finishing drivers and driver-caused retirements\n",
    "        for dri in rnd_dri_scores.keys():\n",
    "            dri_scores[dri] += model.get_rating_change(rnd_dri_scores[dri])\n",
    "\n",
    "    return - log_likelihood / n_pred\n",
    "\n",
    "params = [\n",
    "    32, # K-factor - sensitivity of rating change\n",
    "    400 # C-factor - sensitivity of expected outcome\n",
    "]\n",
    "result = optimize.minimize(model_data, params, method=\"L-BFGS-B\", options={\"disp\": True})\n",
    "\n",
    "# 313266 predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model best parameters: [  7.57423467 398.36297615]\n",
      "Baseline model neg log likelihood: 0.6143407635041344\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline model best parameters: {result.x}\")\n",
    "print(f\"Baseline model neg log likelihood: {model_data(result.x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current system neg log likelihood: 0.6095\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/metrics.json\") as infile:\n",
    "    perf_dict = json.load(infile)\n",
    "\n",
    "print(f\"Current system neg log likelihood: {round(perf_dict['log_likelihood'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1-rating-system-bj5GO79q-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
